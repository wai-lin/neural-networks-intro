{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json_tricks\n",
                "answer = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2018-12-29T10:47:34.674891Z",
                    "start_time": "2018-12-29T10:47:34.664593Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Gradient descent:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here is a theory:\n",
                "- Gradient of the loss function over the weights $\\nabla_{\\mathbf x} L$ points to the direction of the fastest ascend.\n",
                "- Thus, we need to perfrom steps in the opposite direction\n",
                "- Here is the Gradient Descent Algorithm:\n",
                "$\\mathbf w^{t+1} = \\mathbf w^{t} - \\alpha \\nabla_{\\mathbf w} L$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Finding gradient with Torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Torch knows, how to calculate derivative of some value over all the tensors that were used to produce it.\n",
                "To do so, we have to calculate this value (usually it is loss function), and after that we have to call its `backward` method.\n",
                "\n",
                "Here is how it is done:\n",
                "```python\n",
                "x = torch.tensor(10, requires_grad=True)\n",
                "y = torch.tensor(5, requires_grad=True)\n",
                "loss = x + y\n",
                "loss.backward()\n",
                "\n",
                "print(x.grad)\n",
                "print(y.grad)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output should be 1 and 1 because if $L = x + y$, then $\\partial_x L = 1$ and $\\partial_y L = 1$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`requires_grad` parameter indicates that for this value it is necessary to calculate gradient (that probably is obvious, but still important).\n",
                "\n",
                "Note that before you perform backpropagation, the gradient is equal to `None` indicating that it is not initialized and cannot be used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "## YOUR CODE HERE ##\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Something more sophisticated"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the next task we will calculate something slightly more sophisticated. We will calculate gradient for the value\n",
                "\n",
                "$f(X) = 10\\cdot\\sum\\limits_{i, j}x_{ij}^2$\n",
                "\n",
                "Our task is to find\n",
                "\n",
                "$\\partial_X f(X)$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Theoretically it is equal to:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2018-12-29T10:40:39.940404Z",
                    "start_time": "2018-12-29T10:40:39.921237Z"
                },
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "source": [
                "$\\partial_{x_{kl}} f(X) = \\Big( 10\\cdot\\sum\\limits_{i, j}x_{ij}^2\\Big)_{x_{kl}}' = 10 \\cdot\\Big(\\sum\\limits_{i, j}x_{ij}^2\\Big)_{x_{kl}}' $"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "source": [
                "$ \\quad = 10 \\cdot\\sum\\limits_{i, j}\\Big(x_{ij}^2\\Big)_{x_{kl}}' $"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "source": [
                "$ \\quad = 10 \\cdot\\sum\\limits_{i, j}2 x_{ij} \\big(x_{ij}\\big)_{x_{kl}}' $\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "source": [
                "$ \\quad = 10 \\cdot 2 x_{kl} \\cdot 1 = 20 x_{kl} $"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hence, the theoretical derivative is:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "$f(X) = 10\\cdot\\sum\\limits_{i, j}x_{ij}^2$\n",
                "\n",
                "$\\partial_X f(X) = 20 X $"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us try to calculate the result using Torch:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2018-12-29T12:09:55.305661Z",
                    "start_time": "2018-12-29T12:09:55.281215Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "X = torch.tensor(\n",
                "    [[1.,  2.,  3.,  4.],\n",
                "     [5.,  6.,  7.,  8.],\n",
                "     [9., 10., 11., 12.]], requires_grad=True)\n",
                "\n",
                "## YOUR CODE HERE ##\n",
                "\n",
                "answer['X_grad'] = X.grad.clone().numpy()\n",
                "print(X.grad, '<- gradient')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Function order\n",
                "\n",
                "In Torch you may visualize, in which order the functions are there in the graph, example is below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2018-12-29T12:09:25.585911Z",
                    "start_time": "2018-12-29T12:09:25.570876Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "print(function.grad_fn)\n",
                "print(function.grad_fn.next_functions[0][0])\n",
                "print(function.grad_fn.next_functions[0][0].next_functions[0][0])\n",
                "print(function.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Some visualization functions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This function will be useful to see the contours of the function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def show_contours(objective,\n",
                "                  x_lims=[-10.0, 10.0], \n",
                "                  y_lims=[-10.0, 10.0],\n",
                "                  x_ticks=100,\n",
                "                  y_ticks=100):\n",
                "    x_step = (x_lims[1] - x_lims[0]) / x_ticks\n",
                "    y_step = (y_lims[1] - y_lims[0]) / y_ticks\n",
                "    X, Y = np.mgrid[x_lims[0]:x_lims[1]:x_step, y_lims[0]:y_lims[1]:y_step]\n",
                "    res = []\n",
                "    for x_index in range(X.shape[0]):\n",
                "        res.append([])\n",
                "        for y_index in range(X.shape[1]):\n",
                "            x_val = X[x_index, y_index]\n",
                "            y_val = Y[x_index, y_index]\n",
                "            res[-1].append(objective(np.array([[x_val, y_val]]).T))\n",
                "    res = np.array(res)\n",
                "    plt.figure(figsize=(7,7))\n",
                "    plt.contour(X, Y, res, 100)\n",
                "    plt.xlabel('$x_1$')\n",
                "    plt.ylabel('$x_2$')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Programming the Gradient Descent"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will start with optimizing a simple function:\n",
                "\n",
                "$f(x, y) = 10 (x^2 + y^2)$\n",
                "\n",
                "It is a good function to start with as it has minimum at point $(0, 0)$. We should see convergence towards that point\n",
                "\n",
                "## The Task\n",
                "- code the function `function_parabola` that calculates the objective function.\n",
                "- code the gradient descent step with $\\alpha = 0.001$. For that:\n",
                "    - calculate the objective function value\n",
                "    - perform backporpagation\n",
                "    - perform a correct update of `X.data`\n",
                "    - zero the gradient of `X` using `X.grad.zero_()`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "x = torch.tensor(\n",
                "    [8., 8.], requires_grad=True)\n",
                "var_history = []\n",
                "fn_history = []\n",
                "\n",
                "def function_parabola(X):\n",
                "    res = 0\n",
                "    ## YOUR CODE HERE ##\n",
                "    return res\n",
                "\n",
                "def make_gradient_step(function, X):\n",
                "    ...\n",
                "    ## YOUR CODE HERE ##\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Some visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "show_contours(function_parabola)\n",
                "plt.scatter(np.array(var_history)[:,0], np.array(var_history)[:,1], s=10, c='r');"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "plt.figure(figsize=(7,7))\n",
                "plt.plot(fn_history);\n",
                "plt.xlabel('step')\n",
                "plt.ylabel('function value');"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "plt.figure(figsize=(7,7))\n",
                "plt.semilogy(fn_history);\n",
                "plt.xlabel('step')\n",
                "plt.ylabel('function value');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Optimizing a slightly more sophisticated function"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below is another function `function_skewed`. Perform its optimization using the Gradient Descend that you have coded above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "def function_skewed(variable):\n",
                "    gramma = torch.tensor([[1., -1.], [1., 1.]]) @ torch.tensor([[1.0, 0.0], [0.0, 4.0]])\n",
                "    res = 10 * (variable.unsqueeze(0) @ (gramma @ variable.unsqueeze(1))).sum()\n",
                "    return res\n",
                "\n",
                "def function_skewed_np(variable):\n",
                "    gramma = np.array([[1, -1], [1, 1]]) @ np.array([[1.0, 0.0], [0.0, 4.0]])\n",
                "    res = 10 * (variable.transpose(1, 0) @ (gramma @ variable)).sum()\n",
                "    return res\n",
                "\n",
                "show_contours(function_skewed_np)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "x = torch.tensor(\n",
                "    [8., 8.], requires_grad=True)\n",
                "var_history = []\n",
                "fn_history = []\n",
                "\n",
                "for i in range(500):\n",
                "    var_history.append(x.data.cpu().numpy().copy())\n",
                "    fn_history.append(function_skewed(x).data.cpu().numpy().copy())\n",
                "    make_gradient_step(function_skewed, x)\n",
                "\n",
                "answer['var_history_2'] = var_history\n",
                "answer['fn_history_2'] = fn_history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "show_contours(function_skewed_np)\n",
                "plt.scatter(np.array(var_history)[:,0], np.array(var_history)[:,1], s=10, c='r');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to use this"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now you can find minimum of rather sophisticated functions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Torch Optim"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Torch has Optim package that contains a lot of interesting optimizers. The most widely used ones are:\n",
                "- SGD\n",
                "- AdamW\n",
                "\n",
                "We will study the differences between the optimizers a bit later, nevertheless, you can use them both to the functions above"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below your task is to: \n",
                "- create an optimizer `SGD` with learning rate 0.001. This optimizer should optimize variable `x`\n",
                "- optimize the function `function_skewed` with it with the following steps:\n",
                "    1. zero the optimizer's gradients (in case you do not do that, the gradients will accumulate)\n",
                "    2. calculate function value\n",
                "    2. bacpropagate the gradient\n",
                "    3. make optimization step\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.tensor(\n",
                "    [8., 8.], requires_grad=True)\n",
                "var_history = []\n",
                "fn_history = []\n",
                "\n",
                "## YOUR CODE HERE ##\n",
                "\n",
                "for i in range(500):\n",
                "    ## YOUR CODE HERE ##\n",
                "    var_history.append(x.data.cpu().numpy().copy())\n",
                "    fn_history.append(function_result.data.cpu().numpy().copy())\n",
                "\n",
                "answer['var_history_3'] = var_history\n",
                "answer['fn_history_3'] = fn_history\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_contours(function_skewed_np)\n",
                "plt.scatter(np.array(var_history)[:,0], np.array(var_history)[:,1], s=10, c='r');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Lastly, here you should create `AdamW` optimizer with learning rate `1`.\n",
                "Perform the same steps in the optimization cycle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.tensor(\n",
                "    [8., 8.], requires_grad=True)\n",
                "var_history = []\n",
                "fn_history = []\n",
                "\n",
                "## YOUR CODE HERE ##\n",
                "\n",
                "for i in range(500):\n",
                "    ## YOUR CODE HERE ##\n",
                "    var_history.append(x.data.cpu().numpy().copy())\n",
                "    fn_history.append(function_result.data.cpu().numpy().copy())\n",
                "\n",
                "answer['var_history_4'] = var_history\n",
                "answer['fn_history_4'] = fn_history\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_contours(function_skewed_np)\n",
                "plt.scatter(np.array(var_history)[:,0], np.array(var_history)[:,1], s=10, c='r');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Other optimizers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are other optimizers in Torch Optim package, but they are almost never used. \n",
                "\n",
                "You even can code your own optimizer if you dare.\n",
                "\n",
                "Nevertheless, the most widely used optimizer and kind of a default choice for any network is Adam."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusion"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now you know how to do optimization with Torch package"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_tricks.dump(answer, open('.answer.json', 'w'))"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "kernelspec": {
            "display_name": "Python (Container)",
            "language": "python",
            "name": "container_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
